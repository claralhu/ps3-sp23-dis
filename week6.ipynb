{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty and Statistical Significance\n",
    "\n",
    "### PS 3 Week 6 Discussion - Clara Hu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "Today, we'll look at the data on final scores in a large undergraduate class where students were randomly assigned to attend tutoring. The final scores for all of the students in the course were recorded and participation in tutoring was noted. \n",
    "\n",
    "-------\n",
    "\n",
    "**Data Dictionary/Codebook**\n",
    "\n",
    "`Mentored`: `TRUE` = student participated in tutoring, `FALSE` = student did not participate in tutoring\n",
    "\n",
    "`Final`: student score on final\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. \n",
    "library(estimatr)\n",
    "set.seed(25)\n",
    "scores <- read.csv(\"data/final_scores.csv\")\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty\n",
    "\n",
    "How did students do on the Final? Since we have all of the data, I can just take the average of that column (we have all of the information, and this number is set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(scores$Final) # The average score was 62%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd(scores$Final) # Std Dev of scores was 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that I am only able to get data for 30 students. I randomly sampled 30 students and asked them for the difference in their final scores. (We are treating the entire dataset/class as the population, and the 30 students as the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sample <- scores[sample(1:nrow(scores), 30, replace = FALSE),] # sample 30 students without replacement\n",
    "hist(our_sample$Final, col = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the standard deviation of the Final score for our 30 students?\n",
    "sd(our_sample$Final)\n",
    "# and the mean of 30 students:\n",
    "mean(our_sample$Final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the mean of all of the students in the dataset! \n",
    "\n",
    "If we did this 1000 times, what are all of the potential outcomes? Let's take a look at the first 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell, you do not need to understand the code here\n",
    "samples <- list()\n",
    "sample_means <- array()\n",
    "\n",
    "for(i in c(1:1000)){\n",
    "    samples[i] <- list(scores[sample(1:nrow(scores), 30, replace = FALSE),]) # sample 30 students without replacement\n",
    "    sample_means[i] <- mean(samples[[i]]$Final) # calculate their average score\n",
    "}\n",
    "\n",
    "head(sample_means, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the range of outcomes we got with 1000 different samples:\n",
    "min(sample_means)\n",
    "max(sample_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncertainty** is measured by the amount of error/noise in an estimate of the mean or average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a graphical format\n",
    "hist(sample_means)\n",
    "abline(v = mean(scores$Final), col = \"red\", lwd = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error\n",
    "\n",
    "What is the standard error of the samples above? Let's find the standard error using the first sample of Final scores we took (`our_sample$Final`).\n",
    "\n",
    "$$Standard Error = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sd(our_sample[!our_sample$Mentored,]$Final) #standard dev for not tutored\n",
    "s2 = sd(our_sample[!our_sample$Mentored,]$Final) #standard dev for tutored\n",
    "n1 = length(our_sample[!our_sample$Mentored,]$Final) # sample size of not tutored\n",
    "n2 = length(our_sample[!our_sample$Mentored,]$Final) #sample size of tutored\n",
    "\n",
    "std_error = ... #mathematically calculate\n",
    "std_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the value we got for the standard error tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "Back to our original question: Did receiving tutoring affect student performance on the Final? \n",
    "\n",
    "**Null Hypothesis:** Tutoring had no effect on Final Scores- scores for students who did not receive tutoring were the same as scores for students who did receive tutoring.  \n",
    "**Alternative Hypothesis:** Tutoring did have an effect on Final Scores- scores for students who did not receive tutoring were different from scores for students who did receive tutoring.\n",
    "\n",
    "### t- statistic\n",
    "\n",
    "The t-statistic describes how likely an estimate of the size we saw would arise by chance even if there is no treatment effect.\n",
    "\n",
    "Let's compare the average Final score for the students that were tutored in our sample of 30 students to the sample distribution of means if there were no treatment effect (not tutored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the average Final performance for the students that were tutored in our sample\n",
    "mean_sample_tutored = mean(our_sample[our_sample$Mentored,]$Final)\n",
    "mean_sample_tutored #our estimate from the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding means Final score for students not tutored in samples\n",
    "#Run this cell, you do not need to understand the code here\n",
    "\n",
    "no_effect_means <- array() \n",
    "no_effect_scores <- numeric(0)\n",
    "for(i in c(1:1000)){\n",
    "    no_effect_means[i] <- mean(samples[[i]][!samples[[i]]$Mentored,]$Final)\n",
    "    no_effect_scores = c(no_effect_scores, samples[[i]][!samples[[i]]$Mentored,]$Final)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize our results\n",
    "hist(no_effect_means, col = \"gray\")\n",
    "abline(v = mean_sample_tutored, col = \"red\", lwd = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the histogram above tell us about the hypotheses?** \n",
    "\n",
    "Let's confirm by finding the t-statistic.\n",
    "\n",
    "$$ t = \\frac{estimate}{SE} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic = ...\n",
    "t_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our t-statistic mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the difference_in_means() function\n",
    "\n",
    "Today's exercise so far was simply to show the statistical intuition behind statistical processes! In practice, we'll use the `difference_in_means()` function to calculate the effect, t-score, standard error, and p-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compare the results we calculated for `our_sample` to the results evaluated using `difference_in_means()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_means(Final ~ Mentored, our_sample, condition1 = FALSE, condition2 = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets take a look at the `difference_in_means` for the entire `scores` dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_means(Final ~ Mentored, scores, condition1 = FALSE, condition2 = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's interpret the results.** \n",
    "\n",
    "What is the treatment effect? Standard Error? t-statistic? p-value?\n",
    "\n",
    "What do the t-statistic and p-value tell us about the treatment effect? Why? \n",
    "\n",
    "*Take notes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
